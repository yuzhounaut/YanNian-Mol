{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for Lifespan Prediction\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a trained model\n",
    "2. Process new molecules\n",
    "3. Generate predictions\n",
    "4. Visualize predictions\n",
    "\n",
    "This uses the refactored `lifespan_predictor` package for clean inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import from the refactored package\n",
    "from lifespan_predictor.config import Config\n",
    "from lifespan_predictor.data.preprocessing import load_and_clean_csv\n",
    "from lifespan_predictor.data.featurizers import CachedGraphFeaturizer\n",
    "from lifespan_predictor.data.fingerprints import FingerprintGenerator\n",
    "from lifespan_predictor.data.dataset import LifespanDataset\n",
    "from lifespan_predictor.models.predictor import LifespanPredictor\n",
    "from lifespan_predictor.utils.logging import setup_logger\n",
    "from lifespan_predictor.utils.io import load_checkpoint\n",
    "from lifespan_predictor.utils.visualization import plot_predictions\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logger(\"inference\", level=\"INFO\")\n",
    "logger.info(\"Starting inference notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Model\n",
    "\n",
    "Load the configuration that was used during training and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the trained model and configuration\n",
    "model_dir = \"../results\"  # Adjust this to your output directory\n",
    "config_path = os.path.join(model_dir, \"training_config.yaml\")\n",
    "model_path = os.path.join(model_dir, \"best_model.pt\")\n",
    "\n",
    "# Load configuration\n",
    "logger.info(f\"Loading configuration from: {config_path}\")\n",
    "config = Config.from_yaml(config_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if config.device.use_cuda and torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "logger.info(\"Initializing model...\")\n",
    "model = LifespanPredictor(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained weights\n",
    "logger.info(f\"Loading model weights from: {model_path}\")\n",
    "checkpoint = load_checkpoint(model_path, model, device=device)\n",
    "\n",
    "model.eval()\n",
    "logger.info(\"Model loaded successfully\")\n",
    "\n",
    "# Display model info\n",
    "if 'epoch' in checkpoint:\n",
    "    logger.info(f\"Model trained for {checkpoint['epoch']} epochs\")\n",
    "if 'best_metric' in checkpoint:\n",
    "    logger.info(f\"Best validation metric: {checkpoint['best_metric']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process New Molecules\n",
    "\n",
    "Load molecules for inference and process them using the same pipeline as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from CSV file\n",
    "# Uncomment and modify the path to use your own data\n",
    "# inference_csv = \"path/to/your/molecules.csv\"\n",
    "# inference_df = load_and_clean_csv(\n",
    "#     csv_path=inference_csv,\n",
    "#     smiles_column=\"SMILES\",\n",
    "#     label_column=None  # No labels for inference\n",
    "# )\n",
    "\n",
    "# Option 2: Use test data from preprocessing\n",
    "test_data_dir = os.path.join(config.data.output_dir, \"test\")\n",
    "if os.path.exists(test_data_dir):\n",
    "    logger.info(f\"Loading test data from: {test_data_dir}\")\n",
    "    inference_df = pd.read_csv(os.path.join(test_data_dir, \"processed_data.csv\"))\n",
    "else:\n",
    "    # Option 3: Create sample molecules for demonstration\n",
    "    logger.info(\"Creating sample molecules for demonstration\")\n",
    "    sample_smiles = [\n",
    "        \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",  # Ibuprofen\n",
    "        \"CC(=O)Oc1ccccc1C(=O)O\",  # Aspirin\n",
    "        \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",  # Caffeine\n",
    "        \"CC(C)NCC(COc1ccccc1)O\",  # Propranolol\n",
    "        \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\"  # Similar to Ibuprofen\n",
    "    ]\n",
    "    inference_df = pd.DataFrame({config.data.smiles_column: sample_smiles})\n",
    "\n",
    "logger.info(f\"Loaded {len(inference_df)} molecules for inference\")\n",
    "inference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Features for New Molecules\n",
    "\n",
    "Process the molecules using the same featurization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SMILES\n",
    "inference_smiles = inference_df[config.data.smiles_column].tolist()\n",
    "\n",
    "# Initialize featurizers\n",
    "graph_featurizer = CachedGraphFeaturizer(\n",
    "    cache_dir=config.data.graph_features_dir,\n",
    "    max_atoms=config.featurization.max_atoms,\n",
    "    atom_feature_dim=config.featurization.atom_feature_dim,\n",
    "    n_jobs=config.featurization.n_jobs\n",
    ")\n",
    "\n",
    "fp_generator = FingerprintGenerator(\n",
    "    morgan_radius=config.featurization.morgan_radius,\n",
    "    morgan_nbits=config.featurization.morgan_nbits,\n",
    "    rdkit_fp_nbits=config.featurization.rdkit_fp_nbits,\n",
    "    n_jobs=config.featurization.n_jobs\n",
    ")\n",
    "\n",
    "logger.info(\"Featurizers initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graph features\n",
    "logger.info(\"Generating graph features...\")\n",
    "inference_adj, inference_features, _ = graph_featurizer.featurize(\n",
    "    smiles_list=inference_smiles,\n",
    "    labels=None,\n",
    "    force_recompute=False\n",
    ")\n",
    "\n",
    "logger.info(f\"Graph features shape: adj={inference_adj.shape}, features={inference_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fingerprints\n",
    "logger.info(\"Generating fingerprints...\")\n",
    "inference_fp_hashed, inference_fp_nonhashed = fp_generator.generate_fingerprints(\n",
    "    smiles_list=inference_smiles,\n",
    "    cache_dir=config.data.fingerprints_dir\n",
    ")\n",
    "\n",
    "logger.info(f\"Fingerprints shape: hashed={inference_fp_hashed.shape}, non-hashed={inference_fp_nonhashed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "logger.info(\"Creating inference dataset...\")\n",
    "inference_dataset = LifespanDataset(\n",
    "    smiles_list=inference_smiles,\n",
    "    graph_features=(inference_adj, inference_features),\n",
    "    fingerprints=(inference_fp_hashed, inference_fp_nonhashed),\n",
    "    labels=None  # No labels for inference\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "inference_loader = DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "logger.info(f\"Created inference dataset with {len(inference_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions\n",
    "\n",
    "Run inference on the processed molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "logger.info(\"Generating predictions...\")\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in inference_loader:\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch)\n",
    "        \n",
    "        # Apply activation for classification\n",
    "        if config.training.task == 'classification':\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "logger.info(f\"Generated predictions for {len(predictions)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "if config.training.task == 'classification':\n",
    "    inference_df['Probability'] = predictions.flatten()\n",
    "    inference_df['Predicted_Class'] = (predictions.flatten() >= 0.5).astype(int)\n",
    "    inference_df['Confidence'] = np.abs(predictions.flatten() - 0.5) * 2  # 0 to 1 scale\n",
    "else:\n",
    "    inference_df['Predicted_Value'] = predictions.flatten()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE RESULTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(inference_df)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for classification\n",
    "if config.training.task == 'classification':\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Probability distribution\n",
    "    axes[0].hist(predictions.flatten(), bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "    axes[0].set_xlabel('Predicted Probability')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Distribution of Predicted Probabilities')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = inference_df['Predicted_Class'].value_counts().sort_index()\n",
    "    axes[1].bar(class_counts.index, class_counts.values, edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Predicted Class')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Distribution of Predicted Classes')\n",
    "    axes[1].set_xticks([0, 1])\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"inference_predictions.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Prediction plot saved to: {os.path.join(model_dir, 'inference_predictions.png')}\")\n",
    "\n",
    "# Visualization for regression\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.hist(predictions.flatten(), bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Predicted Value')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Distribution of Predicted Values')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"inference_predictions.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Prediction plot saved to: {os.path.join(model_dir, 'inference_predictions.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "output_path = os.path.join(model_dir, \"inference_results.csv\")\n",
    "inference_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Save predictions as numpy array\n",
    "predictions_path = os.path.join(model_dir, \"predictions.npy\")\n",
    "np.save(predictions_path, predictions)\n",
    "logger.info(f\"Predictions array saved to: {predictions_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults saved to: {model_dir}\")\n",
    "print(f\"  - CSV: inference_results.csv\")\n",
    "print(f\"  - Predictions: predictions.npy\")\n",
    "print(f\"  - Visualization: inference_predictions.png\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if config.training.task == 'classification':\n",
    "    print(f\"Total molecules: {len(inference_df)}\")\n",
    "    print(f\"\\nPredicted as Class 0: {(inference_df['Predicted_Class'] == 0).sum()}\")\n",
    "    print(f\"Predicted as Class 1: {(inference_df['Predicted_Class'] == 1).sum()}\")\n",
    "    print(f\"\\nMean probability: {predictions.mean():.4f}\")\n",
    "    print(f\"Std probability: {predictions.std():.4f}\")\n",
    "    print(f\"Min probability: {predictions.min():.4f}\")\n",
    "    print(f\"Max probability: {predictions.max():.4f}\")\n",
    "    print(f\"\\nMean confidence: {inference_df['Confidence'].mean():.4f}\")\n",
    "    \n",
    "    # High confidence predictions\n",
    "    high_conf = inference_df[inference_df['Confidence'] > 0.8]\n",
    "    print(f\"\\nHigh confidence predictions (>0.8): {len(high_conf)}\")\n",
    "    if len(high_conf) > 0:\n",
    "        print(\"\\nTop 5 most confident predictions:\")\n",
    "        print(high_conf.nlargest(5, 'Confidence')[['SMILES', 'Probability', 'Predicted_Class', 'Confidence']])\n",
    "else:\n",
    "    print(f\"Total molecules: {len(inference_df)}\")\n",
    "    print(f\"\\nMean predicted value: {predictions.mean():.4f}\")\n",
    "    print(f\"Std predicted value: {predictions.std():.4f}\")\n",
    "    print(f\"Min predicted value: {predictions.min():.4f}\")\n",
    "    print(f\"Max predicted value: {predictions.max():.4f}\")\n",
    "    print(f\"Median predicted value: {np.median(predictions):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
